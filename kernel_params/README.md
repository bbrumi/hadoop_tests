# Kernel parameters when installing Hadoop


This is from the Hadoop Operations 1e. Watch out because you also need to read the errata.


* **vm.swappiness**The kernel parameter vm.swappiness controls the kernel’s tendency to swap application data from memory to disk, in contrast to discarding filesystem cache. The valid range for vm.swappiness is 0 to 100 where higher values indicate that the kernel should be more aggressive in swapping application data to disk, and lower values defer this be- havior, instead forcing filesystem buffers to be discarded. Swapping Hadoop daemon data to disk can cause operations to timeout and potentially fail if the disk is performing other I/O operations. This is especially dangerous for HBase as Region Servers must maintain communication with ZooKeeper lest they be marked as failed. To avoid this, vm.swappiness should be set to 0 (zero) to instruct the kernel to never swap application data, if there is an option. Most Linux distributions ship with vm.swappiness set to 60 or even as high as 80.
* **vm.overcommit_memory**
Processes commonly allocate memory by calling the function malloc(). The kernel decides if enough RAM is available and either grants or denies the allocation request. Linux (and a few other Unix variants) support the ability to overcommit memory; that is, to permit more memory to be allocated than is available in physical RAM plus swap. This is scary, but sometimes it is necessary since applications commonly allocate memory for “worst case” scenarios but never use it.There are three possible settings for vm.overcommit_memory.
0 (zero)
Check if enough memory is available and, if so, allow the allocation. If there isn’t enough memory, deny the request and return an error to the application. In fact, the kernel permits a "reasonable, minimal amount of overcommit, based on internal (undocumented) heuristics."
1 (one)
The kernel’s equivalent of “all bets are off,” a setting of 1 tells the kernel to always return success to an application’s request for memory. This is absolutely as weird and scary as it sounds.
2 (two)
Permit memory allocation in excess of physical RAM plus swap, as defined by vm.overcommit_ratio. The amount of allocation to permit is actually the total amount of swap space plus R% of the system memory, where R is the value of vm.overcommit_ratio kernel parameter. For example, a system with 1 GB of swap space, 1 GB of physical RAM, and a vm.overcommit_ratio setting of 50 would permit up to 1.5 GB of memory to be allocated.When a process forks, or calls the fork() function, its entire page table is cloned. In other words, the child process has a complete copy of the parent’s memory space, which requires, as you’d expect, twice the amount of RAM. If that child’s intention is to immediately call exec() (which replaces one process with another) the act of cloning the parent’s memory is a waste of time. Because this pattern is so common, the vfork() function was created, which unlikefork(), does not clone the parent memory, instead blocking it until the child either calls exec() or exits. The problem is that the HotSpot JVM developers implemented Java’s fork operation using fork() rather than vfork().So why does this matter to Hadoop? Hadoop Streaming—a library that allows MapReduce jobs to be written in any language that can read from standard in and write to standard out—works by forking the user’s code as a child process and piping data through it. This means that not only do we need to account for the memory the Java child task uses, but also that when it forks, for a moment in time before it execs, it uses twice the amount of memory we’d expect it to. For this reason, it is sometimes necessary to set vm.overcommit_memory to the value 2 and adjust vm.overcommit_ratio accordingly.See also: [Two memory-related issues on the Apache Hadoop cluster (memory swapping and the OOM killer)](http://hakunamapdata.com/two-memory-related-issues-on-the-apache-hadoop-cluster/)